# All ANN-to-SNN Conversion Paper

with *means haven't read it yet.

part from: https://github.com/Jackn0/must-read-papers-on-snn#conversion

[toc]

## lossless via adjusting parameters

1. **Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing--application to feedforward ConvNets**. PAMI 2013. [paper](https://ieeexplore.ieee.org/abstract/document/6497055)

   *Pérez-Carrasco J A, Zhao B, Serrano C, et al.*

1. **Spiking deep convolutional neural networks for energy-efficient object recognition.** IJCV 2015. [paper](https://link.springer.com/content/pdf/10.1007/s11263-014-0788-3.pdf)

   *Y Cao, Y Chen, D Khosla.*

1. **Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.** IJCNN 2015. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.2413&rep=rep1&type=pdf)

   *Diehl P U, Neil D, Binas J, et al.*

1. **Conversion of continuous-valued deep networks to efficient event-driven networks for image classification.** Frontiers in neuroscience 2017. [paper](https://internal-journal.frontiersin.org/articles/10.3389/fnins.2017.00682/full)

   *Rueckauer B, Lungu I A, Hu Y, et al.*

1. **Going deeper in spiking neural networks: VGG and residual architectures.** Frontiers in neuroscience 2019. [paper](https://www.frontiersin.org/articles/10.3389/fnins.2019.00095/full)

   *Sengupta A, Ye Y, Wang R, et al.*

1. **Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network.** CVPR 2020.[paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Han_RMP-SNN_Residual_Membrane_Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.pdf)

   *Han B, Srinivasan G, Roy K.*

1. **(*)Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation.** ICLR 2020. [paper](https://arxiv.org/pdf/2005.01807.pdf)

   *Rathi N, Srinivasan G, Panda P, et al.* 

1. **(*)Exploring the connection between binary and spiking neural networks.** Frontiers in Neuroscience 2020. [paper](https://www.frontiersin.org/articles/10.3389/fnins.2020.00535/full)

   *Lu S, Sengupta A.*

1. **(*)TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers.** arXiv 2020. [paper](https://arxiv.org/pdf/2008.04509.pdf)

   *Ho N D, Chang I J.*

1. **DIET-SNN: A Low-Latency Spiking Neural Network with Direct Input Encoding & Leakage and Threshold Optimization.** Openreview 2020. [paper](https://openreview.net/pdf?id=u_bGm5lrm72)

   *N Rathi, K Roy.*

1. **Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks.** ICLR 2021. [paper](https://arxiv.org/pdf/2103.00476.pdf)

   *Deng S, Gu S.*

1. **Constructing Accurate and Efficient Deep Spiking Neural Networks With Double-Threshold and Augmented Schemes**. TNNLS 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9328869)

   Yu Q, Ma C, Song S, et al. 

1. **(*)Minimizing Inference Time: Optimization Methods for Converted Deep Spiking Neural Networks.** IJCNN 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9533874/)

   *Mueller E, Hansjakob J, Auge D, et al.*

1. **Optimal ann-snn conversion for fast and accurate inference in deep spiking neural networks.** arXiv 2021. [paper](https://arxiv.org/abs/2105.11654)

   *Ding J, Yu Z, Tian Y, et al.*

1. **A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration.** ICML 2021. [paper](https://arxiv.org/pdf/2106.06984.pdf)

    *Li Y, Deng S, Dong X, et al.*

1. **Bsnn: Towards faster and better conversion of artificial neural networks to spiking neural networks with bistable neurons**. arXiv 2021. [paper](https://arxiv.org/abs/2105.12917)

     *Li Y, Zeng Y, Zhao D.*
     
1. **Near Lossless Transfer Learning for spiking neural networks**. AAAI 2021. [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17265)

     *Yan, Z., Zhou, J. Wong, W.-F.*
     

1. **Optimized Potential Initialization for Low-latency Spiking Neural Networks**. AAAI 2022. [paper](https://www.aaai.org/AAAI22Papers/AAAI-3681.BuT.pdf)

    *Bu T, Ding J, Yu Z, et al.*

1. **Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes**. IJCAI 2022. [paper](https://arxiv.org/pdf/2204.13271.pdf)

    *Li Y, Zeng Y*

1. **Spikeconverter: An efficient conversion framework zipping the gap between artificial neural networks and spiking neural networks** AAAI. 2022. [paper](https://www.aaai.org/AAAI22Papers/AAAI-364.LiuF.pdf)

    *Liu F, Zhao W, Chen Y, et al.*

1. **Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks.** ICLR 2022. [paper](https://openreview.net/forum?id=7B3IJMM1k_M)

    *Bu T, Fang W, Ding J, et al.*

1. **Wang Z, Lian S, Zhang Y, et al. Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization**. arXiv 2022. [paper](https://arxiv.org/abs/2205.07473)

    *Wang Z, Lian S, Zhang Y, et al.*

1. **(*)Can deep neural networks be converted to ultra low-latency spiking neural networks? **DATE. 2022 [paper](https://ieeexplore.ieee.org/abstract/document/9774704/)

    *Datta G, Beerel P A.* 

    

## lossless via changing spike coding schemes

1. **Deep neural networks with weighted spikes**. Neurocomputing, 2018 [paper](https://www.sciencedirect.com/science/article/pii/S0925231218306726)

   *Kim J, Kim H, Huh S, et al.* 

1. **(*)Conversion of analog to spiking neural networks using sparse temporal coding.** ISCAS. 2018. [paper](https://ieeexplore.ieee.org/abstract/document/8351295)

   *Rueckauer B, Liu S C.*

1.  **Fast and efficient information transmission with burst spikes in deep spiking neural networks**. DAC. 2019. [paper](https://ieeexplore.ieee.org/abstract/document/8807080)

   *Park S, Kim S, Choe H, et al.*

1. **Deep spiking neural network: Energy efficiency through time based coding.** ECCV 2020. [paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550392.pdf)

   *Han B, Roy K.*
   
   

## lossless via modeling activation functions

1. **(*)Noisy softplus: an activation function that enables snns to be trained as anns** arXiv 2017. [paper](https://arxiv.org/abs/1706.03609)

   *Liu Q, Chen Y, Furber S.*

2. **(*)Improving the antinoise ability of DNNs via a bio-inspired noise adaptive activation function rand softplus** Neural Computation 2019. [paper](https://direct.mit.edu/neco/article-abstract/31/6/1215/8476/Improving-the-Antinoise-Ability-of-DNNs-via-a-Bio)

   *Chen Y, Mai Y, Xiao J, et al.*



## conversion on object detection

1. **Spiking-yolo: spiking neural network for energy-efficient object detection**. AAAI 2020. [paper](https://ojs.aaai.org/index.php/AAAI/article/view/6787)

   *Kim S, Park S, Na B, et al.*

2.  **A fully spiking hybrid neural network for energy-efficient object detection.** TIP 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9591302)

   *Chakraborty B, She X, Mukhopadhyay S.*

# Other Related Paper

## Neuroscience literature Threshold adaptation model

1. **Dynamic spike threshold reveals a mechanism for synaptic coincidence detection in cortical neurons in vivo**. PNAS. [paper](https://www.pnas.org/doi/abs/10.1073/pnas.130200797)

   *Azouz R, Gray C M.*

2. **A threshold equation for action potential initiation**. PLoS computational biology, 2010. [paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000850)

   *Platkiewicz J, Brette R.*

3. **Spike-threshold adaptation predicted by membrane potential dynamics in vivo**. PLoS computational biology, 2014. [paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003560)

   *Fontaine B, Peña J L, Brette R.*

## Conversion on hardware

1.  **(*)Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware**. ICRC 2016. [paper](https://ieeexplore.ieee.org/abstract/document/7738691/)

   *Diehl P U, Zarrella G, Cassidy A, et al.*

